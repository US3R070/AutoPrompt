import logging
import json
from pathlib import Path
import pandas as pd
import wandb
import math
import re
from sklearn.metrics import confusion_matrix
from optimization_pipeline import OptimizationPipeline
from dataset.base_dataset import DatasetBase
from utils.llm_chain import get_llm


class SingleClassifyOptimizationPipeline(OptimizationPipeline):
    def __init__(self, *args, meta_chain=None, few_shot_selector=None, **kwargs):
        super().__init__(*args, **kwargs)
        self.meta_chain = meta_chain
        self.few_shot_selector = few_shot_selector
        # ç¢ºä¿predictorèˆ‡å…¶ä»–æ­¥é©Ÿå…¼å®¹
        self._ensure_predictor_compatibility()

    def _ensure_predictor_compatibility(self):
        """
        ç¢ºä¿predictorèˆ‡å…¶ä»–æ­¥é©Ÿå…¼å®¹
        """
        if hasattr(self, 'predictor') and self.predictor is not None:
            # ç¢ºä¿predictoræœ‰cur_instructå±¬æ€§
            if not hasattr(self.predictor, 'cur_instruct'):
                self.predictor.cur_instruct = None
            
            # ç¢ºä¿predictoræœ‰init_chainæ–¹æ³•ï¼ˆå¦‚æœéœ€è¦çš„è©±ï¼‰
            if not hasattr(self.predictor, 'init_chain'):
                def init_chain(label_schema):
                    # ç©ºå¯¦ç¾ï¼Œå› ç‚ºæˆ‘å€‘ä½¿ç”¨raw prompt
                    pass
                self.predictor.init_chain = init_chain

    def classifier_early_stopping(self):
        """
        Classifier å°ˆç”¨çš„ early stopping æ©Ÿåˆ¶
        ç•¶é”åˆ°å®Œç¾æº–ç¢ºåº¦ï¼ˆ100%ï¼‰æ™‚åœæ­¢è¨“ç·´
        """
        if not self.eval.history:
            return False
        
        # æª¢æŸ¥æœ€æ–°çš„åˆ†æ•¸æ˜¯å¦é”åˆ°å®Œç¾æº–ç¢ºåº¦
        latest_score = self.eval.history[-1]['score']
        
        # å¦‚æœåˆ†æ•¸é”åˆ° 1.0ï¼ˆ100% æº–ç¢ºåº¦ï¼‰ï¼Œå‰‡åœæ­¢
        if latest_score >= 1.0:
            self.log_and_print(f'ğŸ‰ é”åˆ°å®Œç¾æº–ç¢ºåº¦ï¼åˆ†æ•¸: {latest_score:.4f}')
            self.log_and_print('Classifier è¨“ç·´å®Œæˆï¼Œåœæ­¢å„ªåŒ–ã€‚')
            # ç¢ºä¿ç•¶å‰æœ€ä½³çµæœè¢«è¨˜éŒ„
            if hasattr(self, 'output_path') and self.output_path:
                def save_prompt_history(output_dir, batch_id, prompt, score):
                    output_dir = Path(output_dir)
                    output_dir.mkdir(parents=True, exist_ok=True)
                    file_path = output_dir / "best_prompts_history.jsonl"
                    with open(file_path, "a", encoding="utf-8") as f:
                        f.write(json.dumps({
                            "batch_id": batch_id,
                            "prompt": prompt,
                            "score": score
                        }, ensure_ascii=False) + "\n")
                save_prompt_history(self.output_path, self.batch_id, self.eval.history[-1]['prompt'], latest_score)
            
            # èª¿è©¦ä¿¡æ¯ï¼šé¡¯ç¤ºç•¶å‰æœ€ä½³çµæœ
            self.log_and_print(f'Early stopping æ™‚çš„æœ€ä½³ prompt: {self.eval.history[-1]["prompt"]}')
            self.log_and_print(f'Early stopping æ™‚çš„æœ€ä½³åˆ†æ•¸: {latest_score}')
            return True
        
        # æª¢æŸ¥æ˜¯å¦æœ‰é€£çºŒå¤šè¼ªæ²’æœ‰æ”¹å–„
        if len(self.eval.history) >= 5:
            recent_scores = [h['score'] for h in self.eval.history[-5:]]
            if all(abs(recent_scores[i] - recent_scores[i-1]) < 0.001 for i in range(1, len(recent_scores))):
                self.log_and_print(f'é€£çºŒ {len(recent_scores)} è¼ªåˆ†æ•¸ç„¡æ”¹å–„ï¼Œåœæ­¢å„ªåŒ–ã€‚')
                self.log_and_print(f'æœ€è¿‘åˆ†æ•¸: {recent_scores}')
                return True
        
        return False

    def extract_best_prompt(self):
        """
        é‡å¯« extract_best_prompt æ–¹æ³•ï¼Œç¢ºä¿è¿”å›çœŸæ­£çš„æœ€ä½³çµæœ
        ç‰¹åˆ¥è™•ç† early stopping çš„æƒ…æ³
        """
        if not self.eval.history:
            return {'prompt': self.cur_prompt, 'score': 0.0}
        
        # æ‰¾å‡ºåˆ†æ•¸æœ€é«˜çš„è¨˜éŒ„
        best_record = max(self.eval.history, key=lambda x: x['score'])
        
        # èª¿è©¦ä¿¡æ¯
        self.log_and_print(f'Classifier extract_best_prompt:')
        self.log_and_print(f'  - æ­·å²è¨˜éŒ„æ•¸é‡: {len(self.eval.history)}')
        self.log_and_print(f'  - æœ€ä½³åˆ†æ•¸: {best_record["score"]:.4f}')
        self.log_and_print(f'  - æœ€ä½³ prompt: {best_record["prompt"]}')
        
        # é¡¯ç¤ºæ‰€æœ‰æ­·å²è¨˜éŒ„çš„åˆ†æ•¸
        for i, record in enumerate(self.eval.history):
            self.log_and_print(f'  - è¨˜éŒ„ {i}: åˆ†æ•¸={record["score"]:.4f}, prompt={record["prompt"][:50]}...')
        
        return {'prompt': best_record['prompt'], 'score': best_record['score']}

    def step(self, current_iter, total_iter):
        
        generated = False
        
        # ç¢ºä¿predictoræœ‰æ­£ç¢ºçš„åˆå§‹åŒ–
        if not hasattr(self.predictor, 'cur_instruct') or self.predictor.cur_instruct is None:
            self.predictor.cur_instruct = self.cur_prompt
        
        self.log_and_print(f'Starting step {self.batch_id}')
        if len(self.dataset.records) == 0:
            self.log_and_print('Dataset is empty generating initial samples')
            self.generate_initial_samples()
            generated = True
        # print("self.dataset.records : ",self.dataset.records)
        if self.config.use_wandb:
            cur_batch = self.dataset.get_leq(self.batch_id)
            random_subset = cur_batch.sample(n=min(10, len(cur_batch)))[['text']]
            self.wandb_run.log(
                {"Prompt": wandb.Html(f"<p>{self.cur_prompt}</p>"), "Samples": wandb.Table(dataframe=random_subset)},
                step=self.batch_id)

        # æª¢æŸ¥æ˜¯å¦æœ‰ ground truth è³‡æ–™ï¼ˆéåˆæˆè³‡æ–™ï¼‰
        has_ground_truth = len(self.dataset.records) > 0 and 'is_synthetic' in self.dataset.records.columns
        ground_truth_count = 0
        if has_ground_truth:
            ground_truth_count = len(self.dataset.records[self.dataset.records['is_synthetic'] == False])
        
        # åªåœ¨ä»¥ä¸‹æƒ…æ³åŸ·è¡Œ annotatorï¼š
        # 1. batch_id > 0 ä¸”æ²’æœ‰ ground truth è³‡æ–™ï¼Œæˆ–
        # 2. æœ‰æ–°ç”Ÿæˆçš„è³‡æ–™ (generated=True)
        if (self.batch_id > 0 and ground_truth_count == 0) or generated:
            logging.info(f'Running annotator on new samples for batch_id: {self.batch_id}')
            self.annotator.cur_instruct = self.cur_prompt
            records = self.annotator.apply(self.dataset, self.batch_id)
            self.dataset.update(records)
        else:
            if ground_truth_count > 0:
                logging.info(f'Skipping annotator for batch_id: {self.batch_id} - using {ground_truth_count} ground truth samples')
            else:
                logging.info('Skipping annotator for initial dataset (batch_id=0).')

        # Few-shot è©•ä¼°ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
        if self.few_shot_selector:
            logging.info('é–‹å§‹ Few-shot è©•ä¼°...')
            max_num = self.dataset.get_leq(0)[self.dataset.get_leq(0)['is_synthetic'] == False].shape[0]
            # è¨ˆç®—æ‰€æœ‰å¯èƒ½çš„çµ„åˆæ•¸
            print(f"æ‰€æœ‰å¯èƒ½çš„çµ„åˆæ•¸: {math.comb(max_num,self.few_shot_selector.num_shots)}")
            max_num = min(10,math.comb(max_num,self.few_shot_selector.num_shots))
            combinations = self.few_shot_selector.sample_few_shot_combinations(max_combinations=max_num)
            best_few_shot_result = self.few_shot_selector.evaluate_few_shot_combinations(
                self.cur_prompt, combinations, self.predictor, self.eval, self.dataset
            )
            # predictor ç”¨ best few-shot çµ„åˆ
            predictor_prompt = best_few_shot_result['prompt']
            prompt_id = f"step_{self.batch_id}_prompt"
            self.few_shot_selector.save_best_few_shot(prompt_id, best_few_shot_result)
            logging.info(f'Few-shot æœ€ä½³åˆ†æ•¸: {best_few_shot_result["score"]:.4f}')
            # æ–°å¢ï¼šfew-shot raw prompt
            raw_prompt = predictor_prompt
        else:
            predictor_prompt = self.cur_prompt
            raw_prompt = self.cur_prompt

        # ä½¿ç”¨ raw prompt é€²è¡Œé æ¸¬
        logging.info('Running Raw Prompt Evaluation')
        testdata = self.dataset.get_leq(self.batch_id)
        llm = get_llm(self.config.predictor.config.llm)
        raw_predictions = []
        
        # æ·»åŠ é€²åº¦æ¢
        from tqdm import tqdm
        progress_bar = tqdm(total=len(testdata), desc="Processing samples", unit="sample")
        
        for i, row in testdata.iterrows():
            try:
                # few-shot prompt å·²ç¶“åœ¨ raw_prompt
                user_input = row['text']
                prompt = f"{raw_prompt}\n\n {user_input}"
                response = llm.invoke(prompt)  # ç›´æ¥å‚³ str
                
                # åªæŠ“ True/False
                if isinstance(response, dict) and 'text' in response:
                    resp_text = response['text']
                else:
                    resp_text = str(response)
                
                # åªæŠ“ self.config.dataset.label_schema çš„ label
                label_schema = self.config.dataset.label_schema
                pattern = r'(' + '|'.join(re.escape(label) for label in label_schema) + r')'
                match = re.search(pattern, resp_text, re.IGNORECASE)
                label = match.group(1) if match else 'Discarded'
                raw_predictions.append(label)
                
            except Exception as e:
                logging.error(f"Error processing sample {i}: {e}")
                raw_predictions.append('Discarded')
            
            # æ›´æ–°é€²åº¦æ¢
            progress_bar.update(1)
        
        # é—œé–‰é€²åº¦æ¢
        progress_bar.close()
        
        # æ›´æ–°datasetçš„predictionæ¬„ä½
        for i, pred in enumerate(raw_predictions):
            if i < len(self.dataset.records):
                self.dataset.records.iloc[i, self.dataset.records.columns.get_loc('prediction')] = pred
        
        # print("self.dataset.records['prediction'] : \n",self.dataset.records['prediction'])

        # è¨­ç½®evalçš„datasetä¸¦è¨ˆç®—åˆ†æ•¸
        self.eval.dataset = self.dataset.get_leq(self.batch_id)
        score = self.eval.eval_score()  # ç›´æ¥ç²å–è¨ˆç®—çš„åˆ†æ•¸
        logging.info('Calculating Score')
        large_errors = self.eval.extract_errors()
        
        # ä½¿ç”¨ eval.add_history æ–¹æ³•è¨˜éŒ„çµæœ
        # å…ˆè¨­ç½® eval çš„ dataset å’Œ scoreï¼Œç„¶å¾Œèª¿ç”¨ add_history
        self.eval.add_history(raw_prompt, self.task_description)
        
        # èª¿è©¦ä¿¡æ¯ï¼šé¡¯ç¤ºç•¶å‰çµæœ
        self.log_and_print(f'ç•¶å‰ step çµæœ:')
        self.log_and_print(f'  - raw_prompt: {raw_prompt[:100]}...')
        self.log_and_print(f'  - score: {score:.4f}')
        self.log_and_print(f'  - history é•·åº¦: {len(self.eval.history)}')
        
        # ä¿å­˜ prompt historyï¼ˆåœ¨ early stopping æª¢æŸ¥ä¹‹å‰ï¼‰
        def save_prompt_history(output_dir, batch_id, prompt, score):
            output_dir = Path(output_dir)
            output_dir.mkdir(parents=True, exist_ok=True)
            file_path = output_dir / "best_prompts_history.jsonl"
            with open(file_path, "a", encoding="utf-8") as f:
                f.write(json.dumps({
                    "batch_id": batch_id,
                    "prompt": prompt,
                    "score": score
                }, ensure_ascii=False) + "\n")
        save_prompt_history(self.output_path, self.batch_id, raw_prompt, score)
        
        # ====== END ======

        if self.config.use_wandb:
            large_errors = large_errors.sample(n=min(6, len(large_errors)))
            correct_samples = self.eval.extract_correct()
            correct_samples = correct_samples.sample(n=min(6, len(correct_samples)))
            vis_data = pd.concat([large_errors, correct_samples])
            wandb_log_data = {"score": self.eval.history[-1]['score'],
                            "prediction_result": wandb.Table(dataframe=vis_data),
                            'Total usage': self.calc_usage()}
            self.wandb_run.log(wandb_log_data, step=self.batch_id)

        # æª¢æŸ¥ early stopping æ¢ä»¶
        if self.classifier_early_stopping():
            self.log_and_print('Early stopping criteria reached')
            # ç¢ºä¿åœ¨ early stopping æ™‚è¨˜éŒ„ç•¶å‰æœ€ä½³çµæœ
            self.log_and_print(f'Early stopping æ™‚çš„ç•¶å‰çµæœ:')
            self.log_and_print(f'  - ç•¶å‰ prompt: {raw_prompt[:100]}...')
            self.log_and_print(f'  - ç•¶å‰åˆ†æ•¸: {score:.4f}')
            return True
        
        # æª¢æŸ¥åŸæœ‰çš„åœæ­¢æ¢ä»¶
        if self.stop_criteria():
            self.log_and_print('Stop criteria reached')
            return True
        if current_iter != total_iter-1:
            self.run_step_prompt()
        self.save_state()

        return False

    def run_step_prompt(self):
        # ç¢ºä¿predictorçš„cur_instructæœ‰æ­£ç¢ºçš„å€¼
        if not hasattr(self.predictor, 'cur_instruct') or self.predictor.cur_instruct is None:
            self.predictor.cur_instruct = self.cur_prompt
        
        # åªå–ä¸Šä¸€è¼ªï¼ˆç›®å‰æœ€ä½³ promptï¼‰ç”¢ç”Ÿçš„éŒ¯èª¤æ¨£æœ¬
        last_history = [self.eval.history[-1]] if self.eval.history else []
        error_samples = []
        for sample in last_history:
            if 'errors' in sample and isinstance(sample['errors'], pd.DataFrame):
                for _, row in sample['errors'].iterrows():
                    error_samples.append({
                        'input': row.get('text', ''),
                        'prediction': row.get('prediction', ''),
                        'label': row.get('annotation', '')
                    })
        # ç”¨ meta_chain.error_analysis ç”¢ç”Ÿ analysis
        analysis = None
        if self.meta_chain is not None and error_samples:
            # æº–å‚™ error_analysis prompt_input
            large_error_to_str = self.eval.large_error_to_str(last_history[-1]['errors'], self.config.meta_prompts.num_err_prompt)
            prompt_input = {
                'task_description': self.task_description,
                'accuracy': self.eval.mean_score,
                'prompt': self.cur_prompt,
                'failure_cases': large_error_to_str
            }
            print("failure_cases: ",large_error_to_str)
            
            # è‹¥æœ‰ label_schema ä¹Ÿå¸¶å…¥
            if 'label_schema' in self.config.dataset.keys():
                prompt_input['labels'] = json.dumps(self.config.dataset.label_schema)
            # è‹¥æœ‰æ··æ·†çŸ©é™£ä¹Ÿå¸¶å…¥
            if 'confusion_matrix' in last_history[-1]:
                prompt_input['confusion_matrix'] = last_history[-1]['confusion_matrix']
            
            # ç”¨error_analysisæ‰¾å•é¡Œ
            analysis_result = self.meta_chain.error_analysis.invoke(prompt_input)
            analysis = analysis_result['text'] if isinstance(analysis_result, dict) and 'text' in analysis_result else str(analysis_result)
            print("[Error Analysis åˆ†æçµæœ]:\n" + analysis)
        
        # å°‡ analysis å‚³çµ¦ meta_chain.step_prompt_chain
        history_prompt = '\n'.join([self.eval.sample_to_text(sample,
                                                            num_errors_per_label=self.config.meta_prompts.num_err_prompt,
                                                            is_score=True) for sample in last_history])
        prompt_input = {"history": history_prompt, "task_description": self.task_description,
                        'error_analysis': analysis or (last_history[-1].get('analysis', '') if last_history else '')}
        

        if 'label_schema' in self.config.dataset.keys():
            prompt_input["labels"] = json.dumps(self.config.dataset.label_schema)
        prompt_suggestion = self.meta_chain.step_prompt_chain.invoke(prompt_input)
        
        # æª¢æŸ¥predictorçš„cur_instructæ˜¯å¦åŒ…å«NO_THINK
        if hasattr(self.predictor, 'cur_instruct') and self.predictor.cur_instruct and "NO_THINK" in self.predictor.cur_instruct:
            prompt_suggestion['prompt'] = prompt_suggestion['prompt']+"\n/NO_THINK"
        
        if self.meta_chain.step_prompt_chain.llm_config.type == 'google':
            if isinstance(prompt_suggestion, list) and len(prompt_suggestion) == 1:
                prompt_suggestion = prompt_suggestion[0]['args']
        
        self.log_and_print(f'Previous prompt score:{self.eval.mean_score}#########')
        print("Inspecting prompt_suggestion:", prompt_suggestion)
        self.log_and_print(f'Get new prompt:{prompt_suggestion["text"]}')
        self.batch_id += 1
        
        # æ›´æ–°predictorçš„cur_instruct
        if hasattr(self.predictor, 'cur_instruct'):
            self.predictor.cur_instruct = prompt_suggestion['text']
        
        if len(self.dataset) < self.config.dataset.max_samples:
            batch_input = {"num_samples": self.config.meta_prompts.samples_generation_batch,
                           "task_description": self.task_description,
                           "prompt": prompt_suggestion['text']}
            batch_inputs = self.generate_samples_batch(batch_input, self.config.meta_prompts.num_generated_samples,
                                                       self.config.meta_prompts.samples_generation_batch)

            if sum([len(t['errors']) for t in last_history]) > 0:
                history_samples = '\n'.join([self.eval.sample_to_text(sample,
                                                                 num_errors_per_label=self.config.meta_prompts.num_err_samples,
                                                                 is_score=False) for sample in last_history])
                for batch in batch_inputs:
                    extra_samples = self.dataset.sample_records()
                    extra_samples_text = DatasetBase.samples_to_text(extra_samples)
                    batch['history'] = history_samples
                    batch['extra_samples'] = extra_samples_text
            else:
                for batch in batch_inputs:
                    extra_samples = self.dataset.sample_records()
                    extra_samples_text = DatasetBase.samples_to_text(extra_samples)
                    batch['history'] = 'No previous errors information'
                    batch['extra_samples'] = extra_samples_text

            samples_batches = self.meta_chain.step_samples.batch_invoke(batch_inputs,
                                                                         self.config.meta_prompts.num_workers)
            new_samples = [element for sublist in samples_batches for element in sublist['samples']]
            new_samples = self.dataset.remove_duplicates(new_samples)
            self.dataset.add(new_samples, self.batch_id)
            logging.info('Get new samples')
        self.cur_prompt = prompt_suggestion['text'] 