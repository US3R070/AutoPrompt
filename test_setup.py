#!/usr/bin/env python3
"""
æ¸¬è©¦AutoPromptåˆ†é¡ä»»å‹™é…ç½®æ˜¯å¦æ­£ç¢ºè¨­ç½®
"""

import os
import sys
import yaml
from pathlib import Path

def test_dependencies():
    """æ¸¬è©¦å¿…è¦çš„ä¾è³´é …æ˜¯å¦å·²å®‰è£"""
    print("ğŸ” æª¢æŸ¥ä¾è³´é …...")
    required_packages = [
        'langchain',
        'langchain_community', 
        'langchain_openai',
        'pandas',
        'easydict',
        'argilla'
    ]
    
    missing_packages = []
    for package in required_packages:
        try:
            __import__(package)
            print(f"  âœ… {package}")
        except ImportError:
            print(f"  âŒ {package} - æœªå®‰è£")
            missing_packages.append(package)
    
    if missing_packages:
        print(f"\nâŒ ç¼ºå°‘ä¾è³´é …: {missing_packages}")
        print("è«‹é‹è¡Œ: pip install -r requirements.txt")
        return False
    else:
        print("âœ… æ‰€æœ‰ä¾è³´é …å·²å®‰è£")
        return True

def test_config_files():
    """æ¸¬è©¦é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”æ ¼å¼æ­£ç¢º"""
    print("\nğŸ” æª¢æŸ¥é…ç½®æ–‡ä»¶...")
    
    config_files = [
        'config/config_classification.yml',
        'config/llm_env.yml'
    ]
    
    for config_file in config_files:
        if not Path(config_file).exists():
            print(f"  âŒ {config_file} - æ–‡ä»¶ä¸å­˜åœ¨")
            return False
        
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                yaml.safe_load(f)
            print(f"  âœ… {config_file}")
        except yaml.YAMLError as e:
            print(f"  âŒ {config_file} - YAMLæ ¼å¼éŒ¯èª¤: {e}")
            return False
    
    print("âœ… é…ç½®æ–‡ä»¶æª¢æŸ¥é€šé")
    return True

def test_dataset():
    """æ¸¬è©¦æ•¸æ“šé›†æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”æ ¼å¼æ­£ç¢º"""
    print("\nğŸ” æª¢æŸ¥æ•¸æ“šé›†æ–‡ä»¶...")
    
    dataset_file = 'dataset/classification_dataset.csv'
    if not Path(dataset_file).exists():
        print(f"  âŒ {dataset_file} - æ–‡ä»¶ä¸å­˜åœ¨")
        return False
    
    try:
        import pandas as pd
        df = pd.read_csv(dataset_file)
        
        if 'input' not in df.columns or 'answer' not in df.columns:
            print(f"  âŒ {dataset_file} - ç¼ºå°‘å¿…è¦çš„æ¬„ä½(input, answer)")
            return False
        
        print(f"  âœ… {dataset_file} - {len(df)} æ¢è¨˜éŒ„")
        print(f"  âœ… æ¨™ç±¤åˆ†ä½ˆ: {df['answer'].value_counts().to_dict()}")
        return True
        
    except Exception as e:
        print(f"  âŒ {dataset_file} - è®€å–éŒ¯èª¤: {e}")
        return False

def test_prompt_files():
    """æ¸¬è©¦promptæ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
    print("\nğŸ” æª¢æŸ¥promptæ–‡ä»¶...")
    
    prompt_files = [
        'prompts/predictor_completion/prediction.prompt',
        'prompts/predictor_completion/annotation.prompt',
        'prompts/predictor_completion/output_schemes.py'
    ]
    
    for prompt_file in prompt_files:
        if not Path(prompt_file).exists():
            print(f"  âŒ {prompt_file} - æ–‡ä»¶ä¸å­˜åœ¨")
            return False
        print(f"  âœ… {prompt_file}")
    
    print("âœ… Promptæ–‡ä»¶æª¢æŸ¥é€šé")
    return True

def test_ollama_connection():
    """æ¸¬è©¦Ollamaé€£æ¥"""
    print("\nğŸ” æª¢æŸ¥Ollamaé€£æ¥...")
    
    try:
        import requests
        response = requests.get('https://ollama.havenook.com/api/tags', timeout=5)
        if response.status_code == 200:
            models = response.json().get('models', [])
            print(f"  âœ… Ollamaé€£æ¥æ­£å¸¸ - å¯ç”¨æ¨¡å‹: {len(models)}")
            for model in models[:3]:  # é¡¯ç¤ºå‰3å€‹æ¨¡å‹
                print(f"    - {model.get('name', 'Unknown')}")
            return True
        else:
            print(f"  âŒ Ollamaé€£æ¥å¤±æ•— - ç‹€æ…‹ç¢¼: {response.status_code}")
            return False
    except Exception as e:
        print(f"  âš ï¸  ç„¡æ³•é€£æ¥åˆ°Ollama: {e}")
        print("    è«‹ç¢ºä¿Ollamaæœå‹™æ­£åœ¨é‹è¡Œ: ollama serve")
        return False

def test_openai_config():
    """æª¢æŸ¥OpenAIé…ç½®"""
    print("\nğŸ” æª¢æŸ¥OpenAIé…ç½®...")
    
    try:
        with open('config/llm_env.yml', 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        api_key = config.get('openai', {}).get('OPENAI_API_KEY', '')
        if not api_key or api_key == '':
            print("  âš ï¸  OpenAI APIå¯†é‘°æœªè¨­ç½®")
            print("    è«‹åœ¨ config/llm_env.yml ä¸­è¨­ç½® OPENAI_API_KEY")
            return False
        else:
            print("  âœ… OpenAI APIå¯†é‘°å·²è¨­ç½®")
            return True
    except Exception as e:
        print(f"  âŒ è®€å–OpenAIé…ç½®å¤±æ•—: {e}")
        return False

def main():
    print("ğŸš€ AutoPromptåˆ†é¡ä»»å‹™é…ç½®æ¸¬è©¦\n")
    
    tests = [
        ("ä¾è³´é …æª¢æŸ¥", test_dependencies),
        ("é…ç½®æ–‡ä»¶æª¢æŸ¥", test_config_files),
        ("æ•¸æ“šé›†æª¢æŸ¥", test_dataset),
        ("Promptæ–‡ä»¶æª¢æŸ¥", test_prompt_files),
        ("Ollamaé€£æ¥æª¢æŸ¥", test_ollama_connection),
        ("OpenAIé…ç½®æª¢æŸ¥", test_openai_config)
    ]
    
    results = []
    for test_name, test_func in tests:
        try:
            result = test_func()
            results.append((test_name, result))
        except Exception as e:
            print(f"  âŒ {test_name} - åŸ·è¡ŒéŒ¯èª¤: {e}")
            results.append((test_name, False))
    
    print("\n" + "="*50)
    print("ğŸ“Š æ¸¬è©¦çµæœç¸½çµ:")
    print("="*50)
    
    for test_name, result in results:
        status = "âœ… é€šé" if result else "âŒ å¤±æ•—"
        print(f"{test_name}: {status}")
    
    passed = sum(1 for _, result in results if result)
    total = len(results)
    
    print(f"\né€šéç‡: {passed}/{total} ({passed/total*100:.1f}%)")
    
    if passed == total:
        print("\nğŸ‰ æ‰€æœ‰æ¸¬è©¦é€šéï¼æ‚¨å¯ä»¥é–‹å§‹ä½¿ç”¨åˆ†é¡ä»»å‹™äº†ã€‚")
        print("\né‹è¡Œå‘½ä»¤:")
        print("python run_classification_pipeline.py")
    else:
        print("\nâš ï¸  è«‹ä¿®å¾©ä¸Šè¿°å•é¡Œå¾Œå†é‹è¡Œåˆ†é¡ä»»å‹™ã€‚")
    
    return passed == total

if __name__ == "__main__":
    main() 